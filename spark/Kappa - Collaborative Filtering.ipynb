{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "  '--conf spark.cassandra.connection.host=cassandra --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,com.datastax.spark:spark-cassandra-connector_2.11:2.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"BigDataRiver\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "ssc = StreamingContext(sc, 60)\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kafkaStream = KafkaUtils.createDirectStream(ssc, ['bdr'], {\"metadata.broker.list\": 'kafka:9092'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = kafkaStream.map(lambda v: v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split is_purchase column into two\n",
    "separateClicksSchema = StructType([   \n",
    "    StructField(\"purchased_count\", LongType(), False),\n",
    "    StructField(\"clicked_count\", LongType(), False)\n",
    "])\n",
    "\n",
    "def separateClicks(is_purchase):\n",
    "  return (is_purchase, 1-is_purchase)\n",
    "\n",
    "separateClicks_udf = F.udf(separateClicks, separateClicksSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildCFModel(train):\n",
    "    def isProductToRating(productCount, clickCount):\n",
    "        return (productCount * 3.0) + clickCount\n",
    "    \n",
    "    ratings = train.rdd.\\\n",
    "        map(lambda r: Rating(r.user_id, r.product, isProductToRating(r.purchased_count, r.clicked_count)))\n",
    "    rank = 10\n",
    "    numIterations = 20\n",
    "    lambdaFactor = 0.01\n",
    "    alpha = 0.01\n",
    "    seed = 42\n",
    "    return ALS.trainImplicit(ratings, rank, numIterations, alpha, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommendTopProducts(dfModel):\n",
    "        numberOfRecommendationsRequired = 5\n",
    "        rdd = dfModel.recommendProductsForUsers(numberOfRecommendationsRequired)\n",
    "        recommendations = rdd.map(lambda (user,ratings): (user, map(lambda r: r.product, ratings)))\n",
    "        topRecommendationsSchema = StructType([\n",
    "            StructField(\"user_id\", IntegerType(), False),\n",
    "            StructField(\"recommended_products\", ArrayType(IntegerType()), False)\n",
    "        ])\n",
    "        return sql.createDataFrame(recommendations, topRecommendationsSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processStream(rdd):\n",
    "    df = sql.read.json(rdd)\n",
    "    if(len(df.columns)):\n",
    "        #store updated counters in C*\n",
    "        df.withColumn('c', separateClicks_udf(df['is_purchase'])).\\\n",
    "            select(\"user_id\",\"product\",\"c.purchased_count\",\"c.clicked_count\").\\\n",
    "            write.format(\"org.apache.spark.sql.cassandra\").mode('append').\\\n",
    "            options(table=\"users_interests\", keyspace=\"bdr\").save()\n",
    "            \n",
    "        #read all data from C*\n",
    "        usersInterests = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "            options(table=\"users_interests\", keyspace=\"bdr\").load().cache()\n",
    "\n",
    "        dfModel = buildCFModel(usersInterests.select(\"user_id\",\"product\",\"clicked_count\",\"purchased_count\"))\n",
    "        top5 = recommendTopProducts(dfModel)\n",
    "        top5.show()\n",
    "        top5.write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"cf\", keyspace=\"bdr\").save()\n",
    "            \n",
    "        print \"Saved\"\n",
    "    else:\n",
    "        print \"Empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.foreachRDD(lambda rdd: processStream(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
